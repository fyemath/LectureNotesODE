% !TEX root = ../main.tex
\chapter{Linear System of Equations}
\chapterdate{11/22--11/30}

\section{Basics of Linear Algebra}

A system of linear equations (or simply a linear system) consists of $m$ linear equations in $n$ variables:
\begin{equation}\label{eq:linearsystem}
  \begin{aligned}
  a_{11}x_1+a_{12}x_2+\cdots a_{1n}x_n=&b_1\\  
  a_{21}x_1+a_{22}x_2+\cdots a_{2n}x_n=&b_2\\  
  \vdots\\
  a_{m1}x_1+a_{m2}x_2+\cdots a_{mn}x_n=&b_m.  
  \end{aligned}  
\end{equation}

Linear algebra provides an operational way to solve system of linear equations. Here, the operators are matrix operations.

\subsection*{Matrices}

An $m\times n$ matrix $A$ is a number array of the form
\[
  A=
  \begin{pmatrix}
  a_{11} & a_{12} &\cdots & a_{1n}\\
  a_{21} & a_{22} &\cdots & a_{2n}\\
  \vdots & \vdots & \ddots & \vdots\\
  a_{n1} & a_{n2} &\cdots & a_{nn}
\end{pmatrix}.
\]
The element in the $i$-th row and $j$-th column is called the $ij$-th element (or $ij$-th entry).

For simplicity, a matrix is also denoted by its $ij$-th element, for example, the matrix $A$ can be denote by $A=\begin{pmatrix}  a_{ij} \end{pmatrix}$.

An $m\times 1$ matrix is called a column vector. A $1\times n$ matrix is called a row vector. 

\begin{example}
  The matrix
  \[X=\begin{pmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n
  \end{pmatrix}\]
  is a column vector.
\end{example}

Two matrices $A=\begin{pmatrix}
  a_{ij}
\end{pmatrix}$ and $B=\begin{pmatrix}
  b_{ij}
\end{pmatrix}$ are said to be equal if $a_{ij}=b_{ij}$ for $1\le i\le m$ and $1\le j\le n$.

A matrix whose element are all equal to $0$ is denoted by $0$.

Using matrices, the linear system \eqref{eq:linearsystem} can be written as
\[AX=0.\]

The scalar product of a constant $k$ and a matrix $A=\begin{pmatrix}
  a_{ij}
\end{pmatrix}$ is defined as
\[kA=\begin{pmatrix}
  ka_{ij}
\end{pmatrix}.\]

The sum of two matrices $A=\begin{pmatrix}
  a_{ij}
\end{pmatrix}$ and $B=\begin{pmatrix}
  b_{ij}
\end{pmatrix}$ is defined as
\[A+B=\begin{pmatrix}
  a_{ij}+b_{ij}
\end{pmatrix}\]

The produce $AB$ of two matrices $A=\begin{pmatrix}
  a_{ij}
\end{pmatrix}$ and $B=\begin{pmatrix}
  b_{ij}
\end{pmatrix}$ is defined if the number of columns of $A$ equals the number of rows of $B$. If $A$ is a $m\times p$ matrix and $B$ is an $p\times n$ matrix, then the product $AB$ is an $m\times n$ matrix defined as
\[AB=\begin{pmatrix}
  \sum\limits_{k=1}^na_{ik}b_{kj}
\end{pmatrix}.\]

Note that in general $AB\neq BA$.

A square matrix is a matrix with the same number of rows and column. The number of rows or columns of a square matrix is called the order of the matrix.

The diagonal elements of a square matrix $A=\begin{pmatrix}
  a_{ij}
\end{pmatrix}$ are the elements $a_{ij}$ with $i=j$.

A diagonal matrix is a square matrix whose non-diagonal elements are all equal to $0$.

An identity matrix, denoted by $I$, is a diagonal matrix whose diagonal elements are all equal to $1$. It is a straightforward result that $IA=A=AI$ for $A$ and $I$ have the same order.

\subsection*{Determinant}

The determinant $\begin{pmatrix}
  A
\end{pmatrix}$ of a $2\times 2$ matrix
\[A=\begin{pmatrix}
  a_{11} & a_{12}\\
  a_{21} & a_{22}
\end{pmatrix}\]
is defined as
\[
  \det\begin{pmatrix}
  A
\end{pmatrix}=a_{11}a_{22}-a_{12}a_{21}.\]

Geometrically, the determinant represents the signed area of the parallelograms generated by the column vectors of the matrix. In higher dimension, it represents the signed volume of the parallelepiped.

The determinant $\det\begin{pmatrix} A_{ij} \end{pmatrix}$ of the $(n-1)\times (n-1)$ matrix $A_{ij}$ obtained from a $n\times n$ matrix $A$ by removing the $p$-th row and the $k$-th column is called the $ij$-th minor of $A$.

The number $C_{ij}=(-1)^{i+j}\det\begin{pmatrix}A_{\color{blue}{ij}}\end{pmatrix}$ is called the cofactor of the element $a_{ij}$ of $A$.

The determinant $\det\begin{pmatrix}
  A
\end{pmatrix}$ of a $n\times n$ matrix $A=\begin{pmatrix}
  a_{ij}
\end{pmatrix}$ is defined as
\[\det\begin{pmatrix}
  A
\end{pmatrix}=\sum\limits_{k=1}^n a_{1k}C_{1k}.\]

It is a fundamental result in linear algebra that 
\[\det\begin{pmatrix}
  A
\end{pmatrix}=\sum\limits_{k=1}^n a_{ik}C_{ik}\]
for any $1\le i\le n$.

A square matrix $A$ is said to be nonsingular if the determinant is nonzero. Otherwise, it is said to be singular.

\begin{example}
  Find the determinant of the matrix
  \[\begin{pmatrix}
    1 & 2 \\
    3 & 1
  \end{pmatrix}.\]
\end{example}
\begin{solution}
  From the definition, the determinant is
  \[
  \det\begin{pmatrix}
    1 & 2 \\
    3 & 1
  \end{pmatrix} = 1\cdot 1 - 2\cdot 3=1-6=-5. 
  \]
\end{solution}

\subsection*{Inverse Matrices}

A square matrix $B$ is called an inverse of a square matrix $A$ if it satisfies both
\[AB=I \qquad\text{and}\qquad BA=I.\]
A square matrix is called invertible if it has an inverse.

A main result about matrix inverse is the following theorem.
\begin{theorem}
  Let $A$ be a square matrix.
  \begin{enumerate}
    \item $A$ is invertible if and only if $A$ is non-singular.
    \item if $A$ is invertible, then its inverse is unique, which is usually denoted by $A^{-1}$.
  \end{enumerate}
\end{theorem}

\begin{example}
  Determine if the following matrix is invertible:
  \[
    A=
  \begin{pmatrix}
    3 & 6 \\
    2 & 4
  \end{pmatrix}.  
  \]
\end{example}
\begin{solution}
  The determinant of the matrix $A$ is
  \[
    \det\begin{pmatrix}
      3 & 6 \\
      2 & 4
    \end{pmatrix}=3\cdot 4-2\cdot 6=0.
  \]
  So the matrix is not invertible.

  Here is another way to see that it not invertible. Note that
\[\begin{pmatrix}
  3 & 6 \\
  2 & 4
\end{pmatrix}\begin{pmatrix}
  2\\ -1
\end{pmatrix}=\begin{pmatrix}
  0\\0
\end{pmatrix}.\]
If the matrix $A$ is invertible, then multiplying the inverse $A^{-1}$ to both sides yields
\[\begin{pmatrix}
  2\\ -1
\end{pmatrix}=\begin{pmatrix}
  0\\0
\end{pmatrix}\]
which is a contradiction.
\end{solution}

This example highlights a more general result.
\begin{theorem}
The determinant of a matrix with two proportional row or columns is equal to $0$.
\end{theorem}

Finding the inverse matrix of a given matrix is essentially to solve linear systems.

\begin{example}
  Find the inverse matrix of the matrix
  \[
    A=\begin{pmatrix}
    0 & 1\\
    -1 & 0
  \end{pmatrix}.
  \]
\end{example}
\begin{solution}
  Since the determinant $\det\begin{pmatrix} A \end{pmatrix}=1$, this the inverse matrix $A^{-1}$ exists. Write
  \[A^{-1}=\begin{pmatrix}
    a & b\\
    c & d
  \end{pmatrix}.\]
  From the definition, the elements $a$, $b$, $c$ and $d$ satisfies the following equations
  \[\begin{pmatrix}
    0 & 1\\
    -1 & 0
  \end{pmatrix}\begin{pmatrix}
    a & b\\
    c & d
  \end{pmatrix}=\begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}.\]
  Simplifying the product yields
  \[\begin{pmatrix}
    c & d\\
    -a & -b
  \end{pmatrix}=\begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}.\]
  Therefore, $a=0$, $b=-1$, $c=1$ and $d=0$.
  It can be checked that 
  \[\begin{pmatrix}
    0 & -1\\
    1 & 0
  \end{pmatrix}\begin{pmatrix}
    0 & 1\\
    -1 & 0
  \end{pmatrix}=\begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}.
  \]
  Therefore, the inverse matrix of $A$ is
  \[A^{-1}=\begin{pmatrix}
    0 & -1\\
    1 & 0
  \end{pmatrix}.\]
\end{solution}

Applying the method in the above example to the matrix
\[
  A=\begin{pmatrix}
  a_{11} & a_{12}\\
  a_{21} & a_{22}
\end{pmatrix}, \qquad \det\begin{pmatrix}
  A
\end{pmatrix}\ne 0\]
yields
\[
  A^{-1}=\frac{1}{\det\begin{pmatrix}
    A
  \end{pmatrix}}
  \begin{pmatrix}
    a_{22} & -a_{12}\\
    -a_{21} & a_{11}
  \end{pmatrix}.
\]

The transpose of a matrix $A$, denoted by $A_T$, is the matrix whose $ij$-th element is the $ji$-th element of $A$.

For a higher order square matrix, there is also a formula for the inverse.

\begin{theorem}[Cofactor formula]
  The inverse matrix of $A=\begin{pmatrix} a_{ij} \end{pmatrix}$ 
  is
  \[
    A^{-1}=\frac{1}{\det\begin{pmatrix}
    A
  \end{pmatrix}}\begin{pmatrix}
    C_{{\color{red}{ji}}}
  \end{pmatrix},
  \]
  where $C_{ij}$ is the cofactor of the $ij$-th element of $A$.
\end{theorem}

Be aware of the transpose to the matrix $\begin{pmatrix} C_{{\color{red}{ji}}} \end{pmatrix}$.

The cofactor formula is better for theoretical applications. 
In practice, to find the inverse matrix, the \href{https://en.wikipedia.org/wiki/Gaussian_elimination}{Gauss-Jordan elimination} method is frequently used. The idea is to solve $n$ equations together by elimination using row operations:
\begin{enumerate}
  \item interchanging two rows;
  \item multiplying a row by a nonzero number;
  \item adding a scalar multiple of one row to another row.
\end{enumerate}

\begin{example}
  Find the inverse of the matrix
  \[
    \begin{pmatrix}
    2 & 5 \\
    1 & 3
  \end{pmatrix}.
  \]
\end{example}
\begin{solution}
  First form the matrix 
  \[
    \begin{pmatrix}
      2 & 5 &  & 1 & 0\\
      1 & 3 &  & 0 & 1
    \end{pmatrix}.
  \]
  The goal is two use row operations to change the left square to the identity matrix.
  \[
    \begin{pmatrix}
      2 & 5 & | & 1 & 0\\
      1 & 3 & | & 0 & 1
    \end{pmatrix}
    \overset{\text{switch rows}}{\rightarrow} \begin{pmatrix}
      1 & 3 & | & 0 & 1\\
      2 & 5 & | & 1 & 0
    \end{pmatrix}
    \overset{2\text{row} 1 - \text{row} 2}{\rightarrow} \begin{pmatrix}
      1 & 3 & | & 0 & 1\\
      0 & 1 & | & -1 & 2
    \end{pmatrix}
    \overset{\text{row} 1 - 3\text{row} 2}{\rightarrow} \begin{pmatrix}
      1 & 0 & | & 3 & -5\\
      0 & 1 & | & -1 & 2
    \end{pmatrix}
  \]
So the inverse matrix is
\[\begin{pmatrix}
  3 & -5\\
  -1 & 2
\end{pmatrix},\]
which is exactly the one given by the cofactor formula.
\end{solution}

The reason that this method works is that each of the row operations can be viewed as multiply the matrix from the left by a so-called elementary matrix.

The cofactor formula can be used to deduce the Crammer's rule for linear systems.

\begin{theorem}[Crammer's rule]
  Let $AX=B$ be a linear system, where $X$ and $B$ are column vectors and $X$ is the vector of unknowns. If $A$ is invertible, then
  \[X=\begin{pmatrix}
    \frac{\det\begin{pmatrix}
      A(i|B)
    \end{pmatrix}}{\det\begin{pmatrix}
      A
    \end{pmatrix}}
  \end{pmatrix},
  \]
  where $A(i|B)$ is the matrix obtained from $A$ by replacing the $i$-th column by the column vector $B$.
\end{theorem}

\begin{example}
  Solve the linear system
  \[
    \begin{aligned}
    x_1-2x_2 = &1\\
    x_1+x_2 = &4  
    \end{aligned}
  \]
\end{example}
\begin{solution}
  One can solve this linear system using the Gauss elimination method. Here, let's try the Crammer's rule.
  Write
  \[
    A= \begin{pmatrix}
    1 & -2\\
    1 & 1
  \end{pmatrix}, \qquad X=\begin{pmatrix}
    x_1\\x_2
  \end{pmatrix}, \qquad B=\begin{pmatrix}
    1\\4
  \end{pmatrix}.\]
  Then
  \[
    A(1|B)=\begin{pmatrix}
    1 & -2\\
    4 & 1
  \end{pmatrix}\qquad A(2|B)=\begin{pmatrix}
    1 & 1\\
    1 & 4
  \end{pmatrix}.
  \]
  Calculating the determinants of $A$, $A(1|B)$ and $A(2|B)$ yields
  \[
    \det\begin{pmatrix}
    A
  \end{pmatrix}=3, \qquad
  \det\begin{pmatrix}
    A(1|B)
  \end{pmatrix}=9,\qquad
  \det\begin{pmatrix}
    A(2|B)
  \end{pmatrix}=3
  \]
  Therefore,
  \[
  \begin{pmatrix}
    x_1\\ x_2
  \end{pmatrix}  = \frac 13
  \begin{pmatrix}
    9\\ 3
  \end{pmatrix} =
  \begin{pmatrix}
    3 \\ 1
  \end{pmatrix}.
  \]
\end{solution}

\subsection*{Eigenvectors and Eigenvalues}

Let $A$ be a square matrix. A non-zero vector $\vec{v}$ is called an eigenvector of $A$ with the eigenvalue $\lambda$ if they satisfy
\[A\vec{v} = \lambda \vec{v}.\]

A constant $\lambda$ is an eigenvalue with the non-zero eigenvector $\vec{v}$ if and only if 
\[
  (A - \lambda I) \vec{v}=0
\] 
if and only if
\[
\det\begin{pmatrix}
  A-\lambda I
\end{pmatrix}=0. 
\]

The polynomial $p(x)=\det\begin{pmatrix} A-xI \end{pmatrix}$ is called the characteristic polynomial of the matrix $A$. A eigenvalue $\lambda$ is of (algebraic) multiplicity $k$ if $p(x)=(x-\lambda)^kq(x)$ where $q(x)$ is a polynomial had no more factor $x-\lambda$. 

\begin{theorem}
Let $A$ be a square matrix. A constant $\lambda$ is an eigenvalue of $A$ if and only if it is a root of the characteristic polynomial of $A$.
\end{theorem}

For a $2\times 2$ matrix $A$, denote by $\tr(A)$ the sum of diagonal elements of $A$, which is called the trace of $A$, and $\det(A)$ the determinant of $A$, then the characteristic polynomial of $A$ is
\[p(x)=x^2-\tr(A)x+\det(A).\]

There is also a trick to find an eigenvector of a $2\times 2$ matrix $A$.
Suppose that $\lambda$ is an eigenvalue of $A$. If
\[
  A-\lambda I=
\begin{pmatrix}
  a & b\\
  c & d
\end{pmatrix},  
\]
then the vector 
\[\begin{pmatrix}
  -b \\ a
\end{pmatrix} \qquad \text{and}\qquad \begin{pmatrix}
  -d \\  c
\end{pmatrix}\] 
are both eigenvectors if they not zero vectors. Otherwise, $A-\lambda I=0$ and any vector is an eigenvector. The reason is that that the determinant of $A-\lambda I$ is zero implies that $(c, d)$ is a multiple of $(a, b)$. 

\begin{example}
  Find the eigenvalues and their corresponding eigenvectors of the matrix
  \[
  \begin{pmatrix}
    1 & 2 \\
    3 & 2
  \end{pmatrix}.  
  \]
\end{example}
\begin{solution}
  The determinant of the matrix is
  \[
    \det\begin{pmatrix}
      1 & 2 \\
      3 & 2
    \end{pmatrix}=-4.  
  \]
  The trace of the matrix is
  \[1+2=3.\]
  So the characteristic polynomial of the matrix is
  \[
  p(x)=x^2-3x-4. 
  \]
  Solving the equation $p(x)=0$ yields
  \[
    x=-1 \qquad\text{or}\qquad x=4.
  \]
  Therefore, the eigenvalues are $-1$ and $4$.

  Since
  \[
  \begin{pmatrix}
    1 & 2 \\
    3 & 2
  \end{pmatrix}  - (-1)
  \begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}=
  \begin{pmatrix}
    2 & 2\\
    3 & 3\\
  \end{pmatrix}
  \]
  An eigenvector of the eigenvalue $-1$ can be taken to be
  \[\vec{v}=\begin{pmatrix}
 2\\ -2   
  \end{pmatrix},
  \]
  or 
  \[\vec{v}=\begin{pmatrix}
    1\\ -1   
     \end{pmatrix}.
     \]

  A similar calculation implies that
  \[\vec{v}=\begin{pmatrix}
    2\\ 3   
     \end{pmatrix}\]
 is an eigenvector of the eigenvalue $4$.
\end{solution}

The usefulness of eigenvalues and eigenvectors is highlighted by the following theorem.

\begin{theorem}[Diagonolization Theorem]
  Suppose the eigenvalues $\lambda_1$, $\dots$, $\lambda_n$ of a $n\times n$ matrix $A$ are distinct. Let $\vec{v}_i$ be the eigenvector of $\lambda_i$. Write $P=(\vec{v}|_1|\cdots |\vec{v}_n)$. Then
  \[
    PAP^{-1}=
  \begin{pmatrix}
    \lambda_1 & \cdots & 0\\
    \vdots & \ddots & \vdots\\
    0 & \cdots & \lambda_n
  \end{pmatrix}.
  \] 
\end{theorem}

When the characteristic polynomial of the matrix $A$ has a repeated root $\lambda$, more vectors other than  eigenvectors will be needed to form an invertible square matrix $P$.

A vector $\vec{v}$ is called a generalized eigenvector associated to the eigenvalue $\lambda$ of the matrix $A$ if
\[(A-\lambda I)^k\vec{v}=0\qquad \text{for some } k.\]

\begin{theorem}[Jordan Normal Form]
  For a square matrix $A$, there exist a matrix $M$ consists of independent generalized eigenvectors such that
  \[
  MAM^{-1}=
    \begin{pmatrix}
      J_1 & \cdots & 0\\
      \vdots & \ddots & 0\\
      0 & \cdots & J_r
    \end{pmatrix}, 
  \]
  where $J_i$ is a $k\times k$ square matrix in the form
  \[
    J_i=\begin{pmatrix}
      \lambda_{i} & 1 & & \\
      & \lambda_{i} & \ddots & \\
      & & \ddots & 1 \\
      & & & \lambda_{i}
    \end{pmatrix},
  \]
  and $\lambda_i$ is an eigenvalue of order $k$.
\end{theorem}

\begin{example}
  Find the generalized eigenvectors of the matrix
  \[\begin{pmatrix}
    2&1\\
    -1&0
  \end{pmatrix}\]
\end{example}
\begin{solution}
  The trace is $2+0=2$ and the determinant is $2\cdot 0-(-1)\cdot 1=1$. Therefore, the characteristic polynomial is $p(x)=x^2-2x+1$ which has a repeated root $x=1$.

  Subtracting the identity matrix from the given matrix yields
  \[\begin{pmatrix}
    2&1\\
    -1&0
  \end{pmatrix}
  -\begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}=
  \begin{pmatrix}
    1 & 1 \\
    -1 & -1
  \end{pmatrix}
  \]
  So the eigenvector associate to the eigenvalue is
  \[
    \vec{u}=
  \begin{pmatrix}
    -1 \\ 1
  \end{pmatrix}.  
  \]

  A generalized vector in this equation is the a vector $\vec{v}$ that satisfies the following linear system
  \[
    \begin{pmatrix}
    1&1\\
    -1&-1
  \end{pmatrix}
  \vec{v}=\begin{pmatrix}
    -1 \\ 1
  \end{pmatrix}.
  \]
  Solving the linear system yields that
  \[
  \vec{v}=\begin{pmatrix}
    0\\ -1
  \end{pmatrix}.  
  \]
\end{solution}

For $2\times 2$ matrix
\[
A=\begin{pmatrix}
  a & b \\
  c & d
\end{pmatrix}, 
\]
the characteristic polynomial $p(x)=x^2-(a+d)x+\det(A)$ has a repeated root if and only if $x^2-(a+d)x+\det(A)=\left(x-\frac{a+d}{2}\right)^2$. It follows that the repeated root is $\lambda=\frac{a+d}{2}$ and $\left(\frac{a-d}{2}\right)^2=-bc$. Then
\[
A-\lambda I=
\begin{pmatrix}
  \frac{a-d}{2} & b\\
  c & \frac{d-a}{2}
\end{pmatrix}.  
\]
Then 
\[
  \vec{u}=
  \begin{pmatrix}
    -b \\ \frac{a-d}{2}
  \end{pmatrix}
\]
is an eigenvector.

Solving the linear system
\[
  \begin{pmatrix}
    \frac{a-d}{2} & b\\
    c & \frac{d-a}{2}
  \end{pmatrix}
  \begin{pmatrix}
    x \\ y
  \end{pmatrix} =
  \begin{pmatrix}
    -b \\ \frac{a-d}{2}
  \end{pmatrix}
\]
yields that the column vector
\[
  \vec{v}=\vec{v}=\begin{pmatrix}
  0\\ -1
\end{pmatrix}
\]
is a generalized eigenvector.

\subsection*{Calculus of Matrices}

If the elements of a matrix $A$ are functions of a variable $t$, then $A$ is called a matrix of functions of $t$.

A matrix of functions $A(x)=\begin{pmatrix} a_{ij}(x) \end{pmatrix}$ is said to be continuous at $t_0$ if all $a_{ij}(x)$ are continuous at $t_0$. It is differential at $t_0$ if all $a_{ij}(x)$ are differential at $t_0$. We write the derivate of $A(x)$ as
\[
  \dfrac{\D A}{\D t}(x)=A'(x)=\begin{pmatrix}
  a_{ij}'(x)
\end{pmatrix}.
\]

Applying the chain rule to the product of matrix yields
\[
(A(x)B(x))'=A'(x)B(x)+A(x)B'(x).  
\]

Similarly, the integral of a matrix of functions $A(x)=\begin{pmatrix}a_{ij}(x)\end{pmatrix}$ is defined as
\[
  \int_a^b A(x)\D t=\begin{pmatrix}
  \int_a^b a_{ij}(x)\D t
\end{pmatrix}.
\]

The Laplace transform of a matrix function $M(t)$ can be defined similarly as
\[\mathcal{L}(M(t))(s)=\int_0^\infty M(t)e^{-st}\D t.\]

\begin{example}
  Let 
  \[
  A(x)=\begin{pmatrix}
    t & e^t\\
    \sin t & \cos t
  \end{pmatrix}.  
  \]
  Find $A'(x)$, $\int_0^1 A(x)\D t$ and $\mathcal{L}{(A)}$.
\end{example}
\begin{solution}
  From the definition, direct calculations shows that
  \[
    A'(x) = \begin{pmatrix}
      (x)' & (e^t)'\\
      (\sin t)' & (\cos t)'
    \end{pmatrix} 
    = \begin{pmatrix}
      1 & e^t\\
      \cos t & -\sin t
    \end{pmatrix},
  \]
  \[
    \int_0^1 A(x)\D t = \begin{pmatrix}
      \int_0^1 t \D t & \int_0^1 e^t\D t\\
      \int_0^1\sin t\D t & \int_0^1\cos t\D t
    \end{pmatrix}  
    =\begin{pmatrix}
      \frac12 & e-1\\
      \cos 1 -1 & -1
    \end{pmatrix},
  \]
  and
  \[
    \mathcal{L}(A)(s)= \begin{pmatrix}
      \mathcal{L}{(t)} & \mathcal{L}{(e^t)}\\
      \mathcal{L}{(\sin t)} & \mathcal{L}{(\cos t)}
    \end{pmatrix}(s)  
    =\begin{pmatrix}
      \frac{1}{s^2} & \frac{1}{s-1}\\
      \frac{s}{s^2+1} & \frac{1}{s^2+1}
    \end{pmatrix}.
  \]
\end{solution}


\section{Linear Systems}

A system of first-order differential equations of $n$ unknown functions of a single independent variable $x$ is a system of differential equations that can be written as
\[
  \begin{aligned}
    y_{1}'=&f_{1}\left(x, y_{1}, y_{2}, \ldots, y_{n}\right) \\
    y_{2}'=&f_{2}\left(x, y_{1}, y_{2}, \ldots, y_{n}\right) \\
    & \vdots \\
    y_{n}'=&f_{n}\left(x, y_{1}, y_{2}, \ldots, y_{n}\right)
    \end{aligned}  
\]

A system of first-order differential equations is called a \dfn{linear system} if it can be written as
\[
  \begin{aligned}
    y'_1=&a_{11}(x)y_1+a_{12}(x)y_2+\cdots+a_{1n}(x)y_n+f_1(x)\\ 
    y'_2=&a_{21}(x)y_1+a_{22}(x)y_2+\cdots+a_{2n}(x)y_n+f_2(x)\\
    &\vdots\\ 
    y'_n=&a_{n1}(x)y_1+a_{n2}(x)y_2+\cdots+a_{nn}(x)y_n+f_n(x).
  \end{aligned}
\]

One mathematical reason for studying systems is that an $n$-th order differential equation
\[y^{(n)}=f(x, y, y',\cdots, y^{(n-1)})\]
can always be turned into a linear system. Set
\[y_1=y, y_2=y', \cdots, y_n=y^{(n)}.\]
Then the $n$-th order differential equation is equivalent to the linear system
\[
  \begin{aligned}
    y_{1}'=&y_2 \\
    y_{2}'=&y_3\\
    & \vdots \\
    y_{n}'=&f\left(x, y_{1}, y_{2}, \ldots, y_{n}\right).
    \end{aligned}  
\]

In terms of matrices of functions, a linear system can be written as
\[\vec{y}'=A(x)\vec{y}+\vec{f}(x),\]
which suggests the following analogue of the existence and uniqueness theorem of the linear first-order differential equations.

\begin{theorem}[Existence and uniqueness]
  In the linear system $\vec{y}'=A(x)\vec{y}+\vec{f}(x)$,
suppose the coefficient matrix $A(x)$ and the vector function $\vec{f}(x)$ are continuous on $(a,b)$, let $x_0$ be in $(a,b)$, and let $\vec{k}$ be an arbitrary constant $n$-vector. Then the initial value problem
    \[\vec{y}'=A(x)\vec{y}+\vec{f}(x), \quad \vec{y}(x_0)=\vec{k} \nonumber\]
  has a unique solution on $(a,b)$.
\end{theorem}

For convenience and clarity, we focus on linear systems of equations in two
unknown functions:
\begin{equation}\label{eq:hom-linear-two}
  \begin{pmatrix}
    y_1(x) \\ y_2(x)
  \end{pmatrix}'=
  \begin{pmatrix}
    a_1(x) & b_1(x)\\
    a_2(x) & b_2(x)
  \end{pmatrix}
  \begin{pmatrix}
    y_1(x) \\ y_2(x)
  \end{pmatrix}
  +
  \begin{pmatrix}
    f_1(x)\\ f_2(x)
  \end{pmatrix}.
\end{equation}

A linear system is called a \dfn{homogeneous linear system} if 
\[
\begin{pmatrix}
  f_1(x) \\ f_2(x)
\end{pmatrix} = 0 
\]

\begin{example}
  Consider the following linear system
  \[
  \begin{aligned}
    \begin{aligned}
      y_1'=&2y_1 + 4y_2\\ 
      y_2'=&4y_1 + 2y_2.
    \end{aligned}
  \end{aligned}  
  \]

  Rewrite the linear system into matrix form and verify that the vector of functions
  \[
    \vec{y}=
    c_1
    \begin{pmatrix}
      e^{6t} \\ e^{6t}
    \end{pmatrix}
    +c_2
    \begin{pmatrix}
      e^{-2t} \\
      -e^{-2t}
    \end{pmatrix}
  \]
  is a solution.
\end{example}
\begin{solution}
  The linear system can be written in matrix form as follows
  \[
  \begin{pmatrix}
    y_1' \\ y_2'
  \end{pmatrix} =
  \begin{pmatrix}
    2 & 4\\
    4 & 2 
  \end{pmatrix} 
  \begin{pmatrix}
    y_1 \\ y_2
  \end{pmatrix}.
  \]
  The derivative of the vector function $\vec{y}$ is
  \[
    \vec{y}'=
    c_1
    \begin{pmatrix}
      6e^{6t} \\ 6e^{6t}
    \end{pmatrix}
    + c_2
    \begin{pmatrix}
      -2e^{-2t} \\
      2e^{-2t}
    \end{pmatrix}
  \]
  Plugging the vector function into the right hand side of the linear system yields
  \[
    \begin{aligned}
      &\begin{pmatrix}
        2 & 4\\
        4 & 2 
      \end{pmatrix} \left(
        c_1
        \begin{pmatrix}
          e^{6t} \\ e^{6t}
        \end{pmatrix}
        +c_2
        \begin{pmatrix}
          e^{-2t} \\
          -e^{-2t}
        \end{pmatrix}
      \right)\\
      =&
        c_1
        \begin{pmatrix}
          2 & 4\\
          4 & 2 
        \end{pmatrix}
        \begin{pmatrix}
          e^{6t} \\ e^{6t}
        \end{pmatrix}
        +c_2
        \begin{pmatrix}
          2 & 4\\
          4 & 2 
        \end{pmatrix}
        \begin{pmatrix}
          e^{-2t} \\
          -e^{-2t}
        \end{pmatrix}
      \\
      =&
        c_1
        \begin{pmatrix}
          6e^{6t} \\ 6e^{6t}
        \end{pmatrix}
        +c_2
        \begin{pmatrix}
          -2e^{-2t} \\
          2e^{-2t}
        \end{pmatrix}
      \\
      =&
        c_1
        \begin{pmatrix}
          6e^{6t} \\ 
          6e^{6t}
        \end{pmatrix} + 
        c_2
        \begin{pmatrix}
          2e^{-2t} \\
          -2e^{-2t}
        \end{pmatrix}
      \\
    \end{aligned}
  \] 
  Therefore, the vector function $\vec{y}$ is a solution.
\end{solution}

\subsection*{Theorems on linear systems}

Before discussing how to solve linear systems, we first discuss the structure of general solutions.

\begin{theorem}
  If $\vec{u}$ and $\vec{v}$ are two solutions of the homogeneous linear system $\vec{y}'=A\vec{y}$ on $[a, b]$, then
\[c_1\vec{u}+c_2\vec{v}\]
is also a solution on $[a, b]$ for any constants $c_1$ and $c_2$.
\end{theorem}
\begin{proof}
  Since $\vec{u}$ and $\vec{v}$ are solutions, 
  \[\vec{u}'=A\vec{u}\qquad\text{and}\qquad\vec{v}'=A\vec{v}.\]
  Taking the linear combination yields
  \[
    \begin{aligned}
      (c_1\vec{u}+c_2\vec{v})'
      =&c_1\vec{u}'+c_2\vec{v}'\\
      =&c_1A\vec{u}+c_2A\vec{v}\\
      =&A(c_1\vec{u}+c_2\vec{v}).  
    \end{aligned}
  \]
  So $c_1\vec{u}+c_2\vec{v}$ is also a solution.
\end{proof}

Comparing with linear second-order equations which can be converted to a linear system, you may wondering if the general solution is a linear combination of two linearly independent solution. The answer is yes.

Given two vector functions $\vec{u}$ and $\vec{v}$, the Wronskian $W(x)$ of them is defined to be the determinant of the matrix $\begin{pmatrix}
  \vec{u} | \vec{v}
\end{pmatrix}$.

\begin{example}
  Find the Wronskian of the vector functions
  \[
  \vec{u}=\begin{pmatrix}
    e^{2t} \\  e^{2t}
  \end{pmatrix},  \qquad
  \vec{v}=\begin{pmatrix}
    2e^t \\ -e^t
  \end{pmatrix}.
  \]
\end{example}
\begin{solution}
  From the definition, the Wronskian is
  \[
  \det\begin{pmatrix}
    e^{2t} & 2e^t\\
    e^{2t} & -e^t
  \end{pmatrix}=-e^{2t}e^{t}-2e^{2t}e^t=-3e^{3t}.  
  \]
\end{solution}

\begin{theorem}
  If two solutions $\vec{u}$ and $\vec{v}$ of the linear system $\vec{y}'=A\vec{y}$ have a non-vanishing Wronskian on $[a, b]$, then $c_1\vec{u}+c_2\vec{v}$ is a general solution of the linear system.
\end{theorem}
The proof is similar to that of the second order differential equations.
\begin{proof}
  By the uniqueness theorem, it sufficiently to show that $c_1$ and $c_2$ can be chosen to satisfy arbitrary conditions $y_1(x_0)=a$ and $y_2(x_0)=b$.
  If the Wronskian does not vanish on $[a, b]$, then the coefficient matrix of the system of equations
\[
  \begin{aligned}
    c_1 u_1(x_0) + c_2 v_1(x_0)= &a\\ 
    c_1 u_2(x_0) + c_2 v_2(x_0)= &b 
  \end{aligned}
\]
is invertible and hence 
\[
\begin{pmatrix}
  c_1 \\ c_2
\end{pmatrix}=\begin{pmatrix}
  u_1(x_0) & v_1(x_0)\\
  u_2(x_0) & v_2(x_0)\\
\end{pmatrix}^{-1}  \begin{pmatrix}
  a \\ b
\end{pmatrix}.
\]
This proves the theorem.
\end{proof}

Now you may expect more analog properties between Wronskian for linear second-order differential equation and Wronskian for linear systems.

\begin{theorem}
  The Wroskian $W(x)$ of two solutions $\vec{u}$ and $\vec{v}$ on $[a, b]$ of the homogeneous linear system $\vec{y}'=A\vec{y}$ is given by
  \[W(x)=W(x_0)e^{\int_{x_0}^x\tr{A(x)}\D t},\]
  where, $\tr{A(x)}$ is the sum of the diagonal elements.
  In particular, the Wronskian is either identically zero or nonwhere zero.
\end{theorem}
\begin{proof}
  Suppose 
  \[
  u(x)=\begin{pmatrix}
    u_1(x)\\
    u_2(x)
  \end{pmatrix},  \qquad 
  v(x)=\begin{pmatrix}
    v_1(x)\\
    v_2(x)
  \end{pmatrix},  \qquad 
  A(x)=\begin{pmatrix}
    a_1(x) & b_1(x)\\
    a_2(x) & b_2(x)
  \end{pmatrix}.
  \]
  Then
  \[
    \begin{pmatrix}
      u_1'(x) & v_1'(x)\\
      u_2'(x) & v_2'(x)
    \end{pmatrix}
    =\begin{pmatrix}
      a_1(x) & b_1(x)\\
      a_2(x) & b_2(x)
    \end{pmatrix}\begin{pmatrix}
     u_1(x) & v_1(x)\\
     u_2(x) & v_2(x)
    \end{pmatrix},
  \]

Applying \href{https://en.wikipedia.org/wiki/Jacobi\%27s_formula}{Jacobi's formula} yields
\[
  \begin{aligned}
    W'(x)=&W(x)\tr\left(\begin{pmatrix}
    u_1(x) & v_1(x)\\
    u_2(x) & v_2(x)
   \end{pmatrix}^{-1}\begin{pmatrix}
    a_1(x) & b_1(x)\\
    a_2(x) & b_2(x)
  \end{pmatrix}\begin{pmatrix}
   u_1(x) & v_1(x)\\
   u_2(x) & v_2(x)
  \end{pmatrix}\right)\\
  =&W(x)\tr\left(\begin{pmatrix}
    a_1(x) & b_1(x)\\
    a_2(x) & b_2(x)
  \end{pmatrix}\begin{pmatrix}
   u_1(x) & v_1(x)\\
   u_2(x) & v_2(x)
  \end{pmatrix}\begin{pmatrix}
    u_1(x) & v_1(x)\\
    u_2(x) & v_2(x)
   \end{pmatrix}^{-1}\right)\\
   =&W(x)\tr(A(x)).
  \end{aligned}
\]
Solving this separable equation implies,
\[W(x)=W(x_0)e^{\int_{x_0}^x\tr{A(t)}\D t}.\]

Therefore, $W(x)\equiv 0$ if and only if $W(x_0)=0$ for some $x_0$ which implies the second statement.
\end{proof}

It follows directly from the above two theorems that the linear combination of two solutions is a general solution if and only if their Wronskian is non-zero.

\begin{theorem}
  The linear combination of two solutions $\vec{u}$ and $\vec{v}$ on $[a, b]$ of the homogeneous linear system $\vec{y}'=A\vec{y}$ is a general solution if and only if the Wronskian is non-zero.
\end{theorem}

Conceptually, there are equivalent characterization of solutions that form a general solution.

Two solutions $\vec{u}$ and $\vec{v}$ of $\vec{y}'=A\vec{y}$ are called \dfn{linearly independent} if the only constants $c_1$ and $c_2$ such that
\[c_1\vec{u}+c_2\vec{v}=0\]
are zero.

A set of solutions $\{\vec{u}, \vec{v}\}$ of $\vec{y}'=A\vec{y}$ is called \dfn{fundamental set} if every solution can be written as the linear combination of those solutions.

The matrix $\begin{pmatrix}
  \vec{u} \mid \vec{v}
\end{pmatrix}$ is called a \dfn{fundamental matrix} for $\vec{y}'=A\vec{y}$ if the column vectors $\vec{u}$ and $\vec{v}$ are linearly independent.

\begin{example}
  Consider the linear system
  \[\vec{y}' = A\vec{y},\]
  where
  \[A =
  \begin{pmatrix}
     -4 & -3 \\ 6 & 5
  \end{pmatrix}.
  \]
  \begin{enumerate}
    \item 
    Verify that the vector functions
  \[\vec{u}=\begin{pmatrix}
    -e^{2t}\\ 2e^{2t}
  \end{pmatrix} \quad \text{and} \quad \vec{v}=\begin{pmatrix}
    -e^{-t} \\  e^{-t}
  \end{pmatrix}\]
  are solutions of the linear system  on $(-\infty, \infty)$.
  \item 
  Compute the Wronskian of $\vec{u}$ and $\vec{v}$.
  \item 
  Find the general solution.
  \end{enumerate}
\end{example}
\begin{solution}
  For the first two question, it is convenient to work with the matrix
  \[
    F(t)=\begin{pmatrix}
    -e^{2t} & -e^{-t}\\  
    2e^{2t} & e^{-t}
  \end{pmatrix}.
  \]
  The derivative of $F(t)$ is
  \[
    F'(t)=\begin{pmatrix}
      -2e^{2t} & e^{-t}\\  
      4e^{2t} & -e^{-t}
    \end{pmatrix}.
  \]
  The product $A(t)F(t)$ is 
  \[
  A(t)F(t)= 
  \begin{pmatrix}
    -4 & -3 \\ 
    6 & 5
 \end{pmatrix} 
  \begin{pmatrix}
    -e^{2t} & -e^{-t}\\  
    2e^{2t} & e^{-t}
  \end{pmatrix}=
  \begin{pmatrix}
    -2e^{2t} & e^{-t}\\  
    4e^{2t} & -e^{-t}
  \end{pmatrix}
  \]
  Therefore, $F'(t)=A(t)F(t)$ which verifies that $\vec{u}$ and $\vec{v}$ are solutions.

  The Wronskian of $\vec{u}$ and $\vec{v}$ is 
  \[
  W(t)=\begin{pmatrix}
    -e^{2t} & -e^{-t}\\  
    2e^{2t} & e^{-t}
  \end{pmatrix} = -e^{2t}e^{-t}-2e^{2t}(-e^{-t})=e^t. 
  \]
  
  Since $W(t)\not\equiv 0$, the solutions $\vec{u}$ and $\vec{v}$ are linearly independent and the linear combination
  \[c_1\begin{pmatrix}
    -e^{2t}\\ 2e^{2t}
  \end{pmatrix} + 
  c_2 \begin{pmatrix}
    -e^{-t} \\  e^{-t}
  \end{pmatrix}
  \]
  is the general solution.
\end{solution}

We now present the theorem about solutions of non-homogeneous linear systems.

\begin{theorem}
  
  Let $y_p$ be a particular solution of the linear system the linear system $\vec{y}'=A\vec{y}+\vec{f}$ and $y_h$ be the general solution of the homogeneous linear system $\vec{y}'=A\vec{y}.$
  Then $\vec{y}_h+\vec{y}_p$ is the general solution of $\vec{y}'=A\vec{y}+\vec{f}$.
\end{theorem}
\begin{proof}
  The statement follows from the fact that 
  the difference of two solutions of $\vec{y}'=A\vec{y}+\vec{f}$ is a solution of 
  $\vec{y}'=A\vec{y}$.
\end{proof}

\section{Constant Coefficient Homogeneous Linear Systems}

Assume the matrix $A$ of the homogeneous linear system $\vec{y}'=A\vec{y}$ is a matrix of constant functions. If you recall that the equation $y'=cy$ has the exponential solution $y=y(0)e^{ct}$, then it is natural to ask where $\vec{y}=e^{At}\vec{y(0)}$ is a solution of the linear system. This is indeed true if we make sense of $e^{At}$. Recall that $e^x=\sum_{n=0}^\infty \frac{x^n}{n!}$. The matrix exponential $e^{At}$ is defined as
\[e^{At}=\sum_{n=0}^\infty \frac{A^nt^n}{n!}.\]

\begin{theorem}
  The linear system $\vec{y}'=A\vec{y}$ with $\vec{y(0)}=\vec{y_0}$ has the solution
  \[\vec{y}=e^{At}\vec{y_0}.\]
\end{theorem}

The question is how to calculate $e^{At}$. In a special case, say $A$ is a diagonal matrix:
\[A=\begin{pmatrix}
  \lambda_1 & 0\\
  0 & \lambda_2
\end{pmatrix},\]
where $\lambda_1\ne\lambda_2$, the power $A^n$ is also diagonal and
\[A^n=\begin{pmatrix}
  \lambda_1^n & 0\\
  0 & \lambda_2^n
\end{pmatrix},\]
which leads to the fact that
\[e^{At}=\begin{pmatrix}
  e^{\lambda_1t} & 0\\
  0 & e^{\lambda_2t}
\end{pmatrix}.\]
Therefore, 
\[
\begin{pmatrix}
  y_1\\ y_2
\end{pmatrix}=
\begin{pmatrix}
  y_1(0) e^{\lambda_1 t}\\ y_2(0) e^{\lambda_2 t}
\end{pmatrix}
\]
is the solution.

For general cases, there are results from linear algebra that can be used to calculate $e^{At}$.
For a constant coefficient homogeneous linear system of two unknown functions, we have the following results.

\begin{theorem}
  Consider the constant coefficient homogeneous linear system of two unknown functions $\vec{y}'=A\vec{y}$.
\begin{enumerate}[label=\textbf{Case \arabic*:}, align=left]
  \item If $A$ has two distinct real roots $\lambda_1$ and $\lambda_2$, then the general solution is
  \[y=c_1e^{\lambda_1 t}\vec{v_1} + c_2e^{\lambda_2 t}\vec{v_2},\]
where $\vec{v_1}$ and $\vec{v_2}$ are associated eigenvectors of $\lambda_1$ and $\lambda_2$ respectively.

  \item 
  If $A$ has two distinct complex eigenvalues $\lambda_1,\lambda_2=\alpha+i\beta$, then the general solution is
  \[
  \vec{y}=
  c_1e^{\alpha t}(\cos(\beta t)\vec{u}-\sin(\beta t)\vec{v}) + 
  c_2e^{\alpha t}(\cos(\beta t)\vec{v}+ \sin(\beta t)\vec{u})
  \]
  where $\vec{u}\pm \mathrm{i} \vec{v}$ are eigenvectors of $\alpha\pm \mathrm{i}\beta$ respectively.

  \item If $A$ has a repeated eigenvalue $\lambda$, then the general solution is
  \[
    \vec{y}=
    c_1e^{\lambda t}\vec{u} +
    c_2(e^{\lambda t}\vec{v} + te^{\lambda t}\vec{u}),
  \]
  where $\vec{u}$ is an eigenvector of $\lambda$ and $\vec{v}$ is a vector satisfying $(A-\lambda I)\vec{v}=\vec{u}$.
\end{enumerate}
\end{theorem}

\begin{example}
  Solve the linear system
  \[
  \vec{y}'=\begin{pmatrix}
    2 & 1\\
    1 & 2
  \end{pmatrix}\vec{y}  
  \]
\end{example}
\begin{solution}
  The characteristic polynomial is
  \[
  p(x)=x^2-\tr(A)x+\det(A)=x^2-4x+3  
  \]
  which has two roots $x=1$ and $x=3$.
  
  Since
  \[
    \begin{pmatrix}
      2 & 1\\
      1 & 2
    \end{pmatrix}- 1\cdot 
  \begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}  
  =
  \begin{pmatrix}
    1 & 1\\
    1 & 1
  \end{pmatrix},
  \]
  an associated eigenvector is
  \[
  \vec{u}=
  \begin{pmatrix}
    -1 \\ 1
  \end{pmatrix}.  
  \]

  Since
  \[
    \begin{pmatrix}
      2 & 1\\
      1 & 2
    \end{pmatrix}- 3\cdot 
  \begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}  
  =
  \begin{pmatrix}
    -1 & 1\\
    1 & -1
  \end{pmatrix},
  \]
  an associated eigenvector is
  \[
  \vec{v}=
  \begin{pmatrix}
    1 \\ 1
  \end{pmatrix}.  
  \]
Therefore, the general solution is
\[
\vec{y}'=c_1e^t\begin{pmatrix}
  -1 \\ 1
\end{pmatrix}  + c_2e^{3t}
\begin{pmatrix}
  1 \\ 1
\end{pmatrix}.
\]
\end{solution}

\begin{example}
  Solve the linear system
  \[
  \vec{y}'=\begin{pmatrix}
    4 & 6\\
    -3 & -2
  \end{pmatrix}\vec{y}  
  \]
\end{example}
\begin{solution}
  The characteristic polynomial is
  \[
  p(x)=x^2-\tr(A)x+\det(A)=x^2-2x+10=(x-1)^2+9  
  \]
  which has two complex roots $x=1\pm 3\mathrm{i}$.
  
  Since
  \[
    \begin{pmatrix}
      2 & 1\\
      1 & 2
    \end{pmatrix}- (1+3\mathrm{i})\cdot 
  \begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}  
  =
  \begin{pmatrix}
    1-3\mathrm{i} & 1\\
    1 & 1-3\mathrm{i}
  \end{pmatrix},
  \]
  an associated eigenvector is
  \[
  \vec{u}=
  \begin{pmatrix}
    -1 \\ 1-3\mathrm{i}
  \end{pmatrix}=
  \begin{pmatrix}
    -1 \\ 1
  \end{pmatrix}
  +\mathrm{i}\begin{pmatrix}
    0 \\ -3
  \end{pmatrix}
  .  
  \]

Therefore, the general solution is
\[
\vec{y}'=c_1e^t\left(
\cos(3t)
\begin{pmatrix}
  -1 \\ 1
\end{pmatrix} -\sin(3t)
\begin{pmatrix}
  0 \\ -3
\end{pmatrix}
\right) + c_2 e^{t}\left(
  \sin(3t)
  \begin{pmatrix}
    -1 \\ 1
  \end{pmatrix}
+\cos(3t)
\begin{pmatrix}
  0 \\ -3
\end{pmatrix}
\right).
\]
\end{solution}

\begin{example}
  Solve the linear system
  \[
    \vec{y}'=\begin{pmatrix}
      0 & -1\\
      1 & -2
    \end{pmatrix}\vec{y}  
  \]
\end{example}

\begin{solution}
  The characteristic polynomial is
  \[
  p(x)=x^2-\tr(A)x+\det(A)=x^2+2x+1  
  \]
  which has a repeated roots $x=-1$.
  
  Since
  \[
    \begin{pmatrix}
      0 & -1\\
      1 & -2
    \end{pmatrix}- (-1)\cdot 
  \begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}  
  =
  \begin{pmatrix}
    1 & -1\\
    1 & -1
  \end{pmatrix},
  \]
  an associated eigenvector is
  \[
  \vec{u}=
  \begin{pmatrix}
    1 \\ 1
  \end{pmatrix}.  
  \]

  Solving the linear system
  \[
    \begin{pmatrix}
      1 & -1\\
      1 & -1
    \end{pmatrix} 
  \vec{v}  
  =
  \begin{pmatrix}
    1\\
    1
  \end{pmatrix},
  \]
  yields
  \[
  \vec{v}=
  \begin{pmatrix}
    0 \\ -1
  \end{pmatrix}.  
  \]
Therefore, the general solution is
\[
\vec{y}'=c_1e^{-1}\begin{pmatrix}
  1 \\ 1
\end{pmatrix}  + c_2e^{-t}\left(
  \begin{pmatrix}
    0 \\ -1
  \end{pmatrix}
+
t\begin{pmatrix}
  1 \\ 1
\end{pmatrix}\right).
\] 
\end{solution}

\section{Variation of Parameters for Nonhomogeneous Linear Systems*}

For nonhomogeneous linear system $\vec{y}'=A\vec{y}+\vec{f}$, the method of variation of parameters can be used to find a particular solution. Suppose
\[
Y=\begin{pmatrix}
  \vec{y}_1 | \vec{y}_2
\end{pmatrix}  
\]
be a fundamental matrix of the complementary linear system $\vec{y}'=A\vec{y}$, a particular solution $\vec{y}_p$ can be expected in the form
\[
\vec{y}_p=Y\vec{u},  
\]
where $\vec{u}$ is a column vector of functions to be determined. Note that
\[Y'=AY.\]
Plugging $\vec{y}_p$ into $\vec{y}'=A\vec{y}+\vec{f}$ and simplifying the equation yields
\[Y\vec{u}'=\vec{f}.\]
It follows that
\[
\vec{u}=\int Y^{-1}\vec{f} \D t.  
\]

\begin{example}
  Consider the linear system 
  \[
    \vec{y}'=
    \begin{pmatrix}
      1 & 2 \\
      2 & 1
    \end{pmatrix}\vec{y}+
    \begin{pmatrix}
      2e^{4t}\\e^{4t} 
    \end{pmatrix}.
  \]
  \begin{enumerate}
    \item 
    Find a particular solution of the system.
    \item
    Find the general solution.
  \end{enumerate}
\end{example}
\begin{solution}
  Solving the complementary homogeneous linear system
  \[
    \vec{y}'=
    \begin{pmatrix}
      1 & 2 \\
      2 & 1
    \end{pmatrix}\vec{y}  
  \]
  yields a fundamental matrix
  \[
  Y=\begin{pmatrix}
    e^{-t} & e^{3t}\\
    -e^{-t} &  e^{3t}.
  \end{pmatrix}  
  \]
  Since the determinant of $Y$ is
  \[
  \det(Y)  = 2 e^{2t},
  \]
  the inverse matrix of $Y$ is
  \[
  Y^{-1}=\frac{1}{2 e^{2t}}\begin{pmatrix}
    e^{3t} & -e^{3t}\\
    e^{-t} &  e^{-t}
  \end{pmatrix} = \frac{1}{2}\begin{pmatrix}
    e^{t} & -e^{t}\\
    e^{-3t} &  e^{-3t}
  \end{pmatrix}.
  \]
Therefore, 
\[
  Y^{-1}\vec{f}=\frac{1}{2}\begin{pmatrix}
    e^{t} & -e^{t}\\
    e^{-3t} &  e^{-3t}
  \end{pmatrix}\begin{pmatrix}
    2e^{4t}\\e^{4t} 
  \end{pmatrix}=\frac12\begin{pmatrix}
    e^{5t}\\ 3e^{t} 
  \end{pmatrix}
\]
and the parameter vector is
\[
    \vec{u}
    =\frac12\begin{pmatrix}
      \int e^{5t}\D t\\ \int 3e^{t}\D t 
    \end{pmatrix}
    =\begin{pmatrix}
      \frac{1}{10}e^{5t} \\ \frac{3}{2}e^{t}
    \end{pmatrix}.
\]
It follows that a particular solution is
\[
\vec{y}_p=Y\vec{u}= \begin{pmatrix}
  e^{-t} & e^{3t}\\
  -e^{-t} &  e^{3t}
\end{pmatrix}\begin{pmatrix}
  \frac{1}{10}e^{5t} \\ \frac{3}{2}e^{t}
\end{pmatrix}
=\begin{pmatrix}
  \frac{8e^{4t}}{5} \\ \frac{7e^{4t}}{4}
\end{pmatrix}.
\]

Therefore, the general solution is
\[
\vec{y}=Y\vec{c}+\vec{y}_p= \begin{pmatrix}
  e^{-t} & e^{3t}\\
  -e^{-t} &  e^{3t}\end{pmatrix}
  \begin{pmatrix}
    c_1\\ c_2
  \end{pmatrix}
  + \begin{pmatrix}
  \frac{8e^{4t}}{5} \\ \frac{7e^{4t}}{4}
\end{pmatrix}.
\]
\end{solution}

\section{Solving Linear System by Laplace Transform}

Recall that the linear system $\vec{y}'=A\vec{y}$ has the solution $\vec{y}=e^{At}$. One way to find $e^{At}$ is the use the Jordan normal form of $A$. Another method is to use Laplace transform.

Recall that the Laplace transform of the first derivative of a function $y$ is
\[\mathcal{L}(y')=s \mathcal{L}(y)-y(0).\]
Therefore,
\[
\begin{aligned}
  \mathcal{L}((e^{At})')(s)=&\mathcal{L}(e^{At})-e^{A\cdot 0}\\
  \mathcal{L}(Ae^{At})(s)=&\mathcal{L}(e^{At})-I\\
  A\mathcal{L}(e^{At})(s)=&\mathcal{L}(e^{At})-I\\
  (A-sI)\mathcal{L}(e^{At})=&-I\\
  \mathcal{L}(e^{At})=&-(A-sI)^{-1}\\
  \mathcal{L}(e^{At})=&(sI-A)^{-1}.
\end{aligned}  
\]

The matrix $sI-A$ is known as the characteristic matrix of the matrix $A$.

\begin{example}
  Find the general solution of the linear system
  \[\vec{y}'=\begin{pmatrix}
    1 & -1\\
    0 & 1
  \end{pmatrix}\vec{y}
  \]
\end{example}
\begin{solution}
  The characteristic matrix is
  \[
    s\begin{pmatrix}
    1 & 0\\ 0 & 1
  \end{pmatrix} \begin{pmatrix}
    1 & -1\\
    0 & 1
  \end{pmatrix}=
  \begin{pmatrix}
    s-1 & 1 \\ 0 & s-1
  \end{pmatrix}.
  \]
  The determinant of the characteristic matrix is $(s-1)^2$. So the inverse matrix is
  \[
    \begin{pmatrix}
    s-1 & 1 \\ 0 & s-1
  \end{pmatrix}  = 
  \begin{pmatrix}
    \frac{1}{s-1} & 0 \\
    -\frac{1}{(s-1)^2} & \frac{1}{s-1}
  \end{pmatrix}.
  \]
 Therefore, the fundamental matrix $e^{At}$ is 
 \[
 \mathcal{L}^{-1}\left(\begin{pmatrix}
  \frac{1}{s-1} & 0 \\
  -\frac{1}{(s-1)^2} & \frac{1}{s-1}
\end{pmatrix}
\right)=
\begin{pmatrix}
  e^t & 0 \\
  -te^t & e^t.
\end{pmatrix}  
 \]
 The general solution is
 \[\vec{y}=\begin{pmatrix}
  e^t & 0 \\
  -te^t & e^t.
\end{pmatrix}\begin{pmatrix}
  c_1 \\ c_2
\end{pmatrix}.
\]
\end{solution}
